<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>重读 Raft 论文 | bloging</title>
    <meta name="description" content="some notes about cs">
    <link rel="icon" href="/blogs/img/logo.ico">
  <link rel="manifest" href="/blogs/manifest.json">
    
    <link rel="preload" href="/blogs/assets/css/0.styles.8b9e79ee.css" as="style"><link rel="preload" href="/blogs/assets/js/app.aa15e581.js" as="script"><link rel="preload" href="/blogs/assets/js/2.b2c8d346.js" as="script"><link rel="preload" href="/blogs/assets/js/3.5b8eab2c.js" as="script"><link rel="prefetch" href="/blogs/assets/js/10.ba4c31f4.js"><link rel="prefetch" href="/blogs/assets/js/11.b4a8a674.js"><link rel="prefetch" href="/blogs/assets/js/12.29ad8b2d.js"><link rel="prefetch" href="/blogs/assets/js/4.2d02bb5f.js"><link rel="prefetch" href="/blogs/assets/js/5.638d79b0.js"><link rel="prefetch" href="/blogs/assets/js/6.f9d58ded.js"><link rel="prefetch" href="/blogs/assets/js/7.eec15176.js"><link rel="prefetch" href="/blogs/assets/js/8.f7f10563.js"><link rel="prefetch" href="/blogs/assets/js/9.9d1a03b3.js">
    <link rel="stylesheet" href="/blogs/assets/css/0.styles.8b9e79ee.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><article id="post"><div class="content default"><h1 id="重读-raft-论文">重读 Raft 论文</h1> <h3 id="安全">安全</h3> <p>如果一台机器<strong>应用</strong>了一条日志，那么其他的机器就无法<strong>应用</strong>具有相同ID但不同命令的日志。</p> <p>Election Safety:</p> <blockquote><p>At most one leader can be elected in a given term. §3.4</p></blockquote> <p>Leader Append-Only:</p> <blockquote><p>A leader never overwrites or deletes entries in its log; it only appends new entries. §3.5</p></blockquote> <p>Log Matching:</p> <blockquote><p>If two logs contain an entry with the same index and term, then the logs are identical in all entries up through the given index. §3.5</p></blockquote> <p>Leader Completeness:</p> <blockquote><p>If a log entry is committed in a given term, then that entry will be present in the logs of the leaders for all higher-numbered terms. §3.6</p></blockquote> <p>State Machine Safety:</p> <blockquote><p><strong>Most Important</strong>:
If a server has applied a log entry at a given index to its state machine,
no other server will ever apply a different log entry for the same index.§3.6.3</p></blockquote> <h3 id="_3-3-基本说明">3.3 基本说明</h3> <p>两种操作：</p> <ul><li><p>RequestVote</p></li> <li><p>AppendEntries</p></li></ul> <h3 id="_3-4-选举">3.4 选举</h3> <p><strong>心跳机制触发选举:</strong>
节点启动时，是 Follower 状态。
如果一直都能收到来自 Candidate 或者 Leader 的请求，那么就会保持在  Follower 状态。
如果在一个随机的时间内，没有收到任何请求，节点就会假设现在没有合适的Leader，它就会发起一个 ReqeustVotes 请求给其他节点，要求成为 Leader。这个时候的节点状态称作 Candidate。</p> <p>这会有三种结果：</p> <ul><li>顺利成为 Leader。（得到了其他节点中半数的同意，再加上自己给自己投的一票，于是大多数同意）</li> <li>有其他节点抢先成为了 Leader。</li> <li>在这个选举周期内，没人成为 Leader。（选举周期是本节点随机出来的一个时间）</li></ul> <p><strong>选举周期：</strong>
当 Candidate 向其他节点发出了 RequestVotes 请求，它会随机等待一段时间，这段时间就是选举周期。</p> <p><strong>节点投票规则：</strong>
在一个选举周期上，只能投票一次，按照谁先来，先投谁的顺序。而且只有当收到的 RequestVote 请求的 term 大于或者等于节点目前已知的 term 。如果小于，就忽略掉。（Section 3.6 添加另外一个限制）</p> <p><strong>节点获选成为 Leader 的规则：</strong>
获得大多数节点的投票同意。保证 <strong><u>Election Safty</u></strong></p> <h3 id="_3-5-日志复制">3.5 日志复制</h3> <p>leader 接收到客户端的命令后，先利用当前的 term 构建一个 entry，append 到自己的 log 里(<strong><u>Leader Append Only</u></strong>)。
同时向其他节点发送 AppendEntries RPC，当有半数以上的节点响应了，leader 就 commit 这个 entry，并把自己 <code>commitedIndex</code> 设置成这个 entry 在 log 中的 index。
在 AppendEntries RPC 中，leader 会把自己当前的 <code>commitedIndex</code> 附带进去， followers 一旦收到它，知晓了已经 commit 到哪个 index ， 就会把当前未 commit 的 entries 按照 index 顺序应用到自己的状态机上。
这样能知道下面两个性质成立，前面提到 <strong><u>Log Matching</u></strong> 性质也就成立了：</p> <ul><li>如果两个不同节点里的log包含index 和 term 都相同的 entry，那么这两个 entry 存储的命令也会是一样的。</li> <li>如果两个不同节点里的log包含 index 和 term 都相同的 entry，那么两个 log 在这个 entry 之前的数据也都一样。</li></ul> <p>但收到 AppendEntries 的 follower 该怎么办呢？
这里需要提一下，在 AppendEntries RPC 中，leader 会把所要 append 的 entries 之前的一个 entry 的标识（即 index 和 term）附带进去。
当 follower 收到它的时候，检查自己的 log 最后是否包含具有相同 index+term 的 entry，（所谓一致性检测，consistency check），如果有，则返回成功。（这样 leader 知道这个 follower 的日志从开始到最新的这个 entry 结束和它自己是一样的）</p> <h4 id="日志追赶">日志追赶</h4> <p>但是 follower 的日志并不总是和 leader 一样。和 leader 的日志相比，follower 的可能漏了一些，也可能多了一些。
这时候就需要做日志追赶操作。
找出 follower 和 leader 都有的日志所在的 index，leader 把这个 index 之后的 entries 通过 AppendEntries 复制到 follower，
而 follower 把它自己日志从这个 index 之后开始 truncate 掉，以接收来自 leader 的最新的日志数据。
<u>TODO(具体实现步骤)</u>
不断重复这个操作，直到 follower 的日志赶上 leader ，达到了 consistency。
leader 发现 follower 和自己一致后，就可以发送心跳信息来保持同步。</p> <p>注意 <u><strong>Leader Append-Only Property</strong></u></p> <h3 id="_3-6-一致性-safety">3.6 一致性 (Safety)</h3> <h4 id="_3-6-1-选举投票限制">3.6.1 选举投票限制</h4> <p>candidate 在成为 leader 之前，  它的日志必须能够涵盖集群中大多数节点的日志。
不然，在之后的日志复制时，没有被涵盖的节点（这种节点占大多数）的日志就会被覆盖掉（<strong>日志追赶</strong>），数据就会出现不一致。
因此节点在处理 RequestVotes 请求时，还需要判断自己的日志是不是比发出这个请求的 candidate 新。
（根据 term/index 比较，term 大的日志更新，term 相同的，index 大的更新）</p> <p>这个规则补充了 Section 3.4 的节点投票规则。</p> <h4 id="_3-6-2-commit-前任留下来的-entries">3.6.2 Commit 前任留下来的 entries</h4> <p>当一个 leader 在将 entry 复制到大多数节点后，但是在 commit 之前挂掉了，
后继的 leader 如果在自己的任期内做过什么“成绩”（成功的将一个 entry 复制到大多数节点上），
那么它不能 commit 前任留下的 entries，否则会出现不一致行为。</p> <p><img src="/blogs/assets/img/commit_entries_from_prev_leader.a1ec8997.png" alt="commit entries from prev leader"></p> <p>因此，只有当 leader 成功的将一个 entry 复制到多数节点上，才能 commit 前任留下来的entries。</p> <h4 id="_3-6-3-一致性证明">3.6.3 一致性证明</h4> <p>State Machine Safety Property</p> <h3 id="_3-7-follower-和-candidate-崩溃的情况">3.7 Follower 和 Candidate 崩溃的情况</h3> <p>如果 follower 或者 candidate 节点崩溃了， 那么 leader 会不断的重试 RPC。节点再次上线的时候，RPC 就能成功。</p> <h3 id="_3-8-持久化状态以及节点重启">3.8 持久化状态以及节点重启</h3> <p>需要持久化的状态，防止在一个任期里投票两次：</p> <ul><li>currentTerm</li> <li>voteFor</li></ul> <p>另外 log entries 是需要持久化的。其他状态可以在重启之后再创建。
尤其是 <strong>commitIndex</strong>，启动时可以初始化成 0 。
当集群内有 Leader 产生，并且之后它 commit 了一个新的 entry，那它的 commitIndex 就会更新，然后就可以迅速在集群内传播。</p> <p>状态机可以是持久化的，也可以是易失的。</p> <ul><li>如果是易失的，那它必须在重启之后通过重放 log entries 来恢复。<strong>（Section 5：snapshot）</strong></li> <li>如果是持久化的，那必须同时持久化最近应用的 log entry 的 index （last_applied_index）。</li></ul> <p>如果有机器丢失了持久化数据，那这台机器只能通过 <strong>Chapter4: cluster membership change</strong> 来重新加入集群。
如果大多数节点都丢了数据，只能人工介入。</p> <h3 id="_3-9-timing-and-availability">3.9 Timing and Availability</h3> <p>选举时候的超时时间设置：</p> <div class="language- extra-class"><pre class="language-text"><code>broadcastTime &lt;&lt; electionTimeout &lt;&lt; MTBF

broadcastTime: 机器之间一个RPC通信的平均时间, 0.5 ~ 20ms。
MTBF: mean time between failures for a single server, 几个月或者更长。
</code></pre></div><p>选举超时时间一个合适区间是 10~500ms。参考 Chapter9。</p> <h3 id="_3-10-leader-主动让权">3.10 leader 主动让权</h3> <p>描述了 leader 主动让权时的情况。</p> <p>周期性的检查哪个follower 更适合当leader；
选中一个之后，停止接受新请求，把自己的 log entries 复制给它；
完毕之后，给它发送 TimeoutNow 的命令（相当于 electionTimeout），
然后进入正常的选举流程。</p> <p>然而这里面涉及到处理<strong>选举失败，leader 需要重新接受请求</strong>的情况。</p> <h2 id="_4-集群节点变更">4. 集群节点变更</h2> <p>将某个节点下线或者，向集群中添加新的节点。</p> <p>扩展现有的raft协议：</p> <ul><li>AddServer</li> <li>RemoteServer</li></ul> <p>Section 4.1:</p> <ul><li>等待之前的 config 被 commit,</li> <li>commit 新的 config</li></ul> <h3 id="_4-2-可用性">4.2 可用性</h3> <ol><li>在把新节点加入集群之前，让它先追一会 log。 4.2.1</li> <li>如果 leader 被从集群中移除了，如何淘汰它。 4.2.2</li> <li>防止被移除的节点破坏新集群的leader。 4.2.3</li> <li>总结，为什么这种节点变更算法足以保证可用性。</li></ol> <h4 id="_4-2-1-catching-up-new-servers">4.2.1 Catching up new servers</h4> <ul><li><p>为什么要在把新节点加入集群之前，让它先追一会日志？</p> <blockquote><p>Attempting to add a server that is unavailable or slow is often a mistake.</p></blockquote></li> <li><p>如何判断新节点的日志是否足够成为 Follower？</p> <p>算法：每一轮复制 leader 所有未复制的日志。要知道在复制的过程中，leader 还在接收新的日志。
当某一轮（必须小于一个规定的最大次数）的日志复制耗时不超过 election timeout，那就可以认为这个新节点日志足够了。</p></li> <li><p>leader 针对新节点的 nextIndex 肯定会回退到起始值 1。</p> <p>为了加快这一进程，follower 可以在 AppendEntries RPC 中返回自己当前的日志 index，leader 就可以一次到位了。
优化 <strong>Section 3.5 日志追赶</strong></p></li></ul> <h4 id="_4-2-2-removing-the-current-leader">4.2.2 Removing the current leader</h4> <p>很直观的方式就是使用 <strong>3.10 leader 主动让权</strong>。</p> <p>这一部分主要介绍的是在这之前使用的一个方式，和 multi-servers configuration change 有关。</p> <h4 id="_4-2-3-disruptive-servers">4.2.3 Disruptive servers</h4> <p>防止被抛弃的几点扰乱新的集群。（因为接收不到心跳的旧节点会不断的 timeout，发起term 比当前term大的 RequestVotes 请求，当然这个请求不会被接受，但会导致新集群不断的选举新的 leader）</p> <p>解决方式：follower 如果在 minimum election_timeout 内接收到了 RequestVote 请求，直接忽略它。</p> <p>另：Pre-Vote （<strong>Section 9</strong>）</p> <h4 id="_4-2-4-availability-argument">4.2.4 Availability argument</h4> <p>STATUS: unread</p> <h3 id="_4-3-任意的配置更新策略">4.3 任意的配置更新策略</h3> <p>STATUS: unread</p> <h3 id="_4-4-系统集成">4.4 系统集成</h3> <ul><li><p>涉及到多次变更，添加节点先于删除节点进行。</p></li> <li><blockquote><p>Membership changes also necessitate a dynamic approach for clients to ﬁnd the cluster; this is discussed in <strong><u>Chapter 6.</u></strong></p></blockquote></li> <li><p>[ ] 集群配置变更的 BUG：<a href="https://groups.google.com/forum/#!msg/raft-dev/t4xj6dJTP6E/d2D9LrWRza8J" target="_blank" rel="noopener noreferrer">raft-dev post<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p></li></ul></div></article><div class="global-ui"></div></div>
    <script src="/blogs/assets/js/app.aa15e581.js" defer></script><script src="/blogs/assets/js/2.b2c8d346.js" defer></script><script src="/blogs/assets/js/3.5b8eab2c.js" defer></script>
  </body>
</html>
